{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f855f631",
   "metadata": {},
   "source": [
    "# Arabic Named-Entity Recognition (NER) — Assignment\n",
    "\n",
    "This notebook guides you through building an Arabic NER model using the ANERCorp dataset (`asas-ai/ANERCorp`). Fill in the TODO cells to complete the exercise.\n",
    "\n",
    "- **Objective:** Train a token-classification model (NER) that labels tokens with entity tags (e.g., people, locations, organizations).\n",
    "- **Dataset:** `asas-ai/ANERCorp` — contains tokenized Arabic text and tag sequences.\n",
    "- **Typical Labels:** `B-PER`, `I-PER` (person), `B-LOC`, `I-LOC` (location), `B-ORG`, `I-ORG` (organization), and `O` (outside/no entity). Your code should extract the exact label set from the dataset and build `label_list`, `id2label`, and `label2id` mappings.\n",
    "- **Key Steps (what you will implement):**\n",
    "  1. Load the dataset and inspect samples.\n",
    "  2. Convert the provided words into sentence groupings (use `.` `?` `!` as sentence delimiters) before tokenization so sentence boundaries are preserved.\n",
    "  3. Tokenize with a pretrained Arabic tokenizer and align tokenized sub-words with original labels (use `-100` for tokens to ignore in loss).\n",
    "  4. Prepare `tokenized_datasets` and data collator for dynamic padding.\n",
    "  5. Configure and run model training using `AutoModelForTokenClassification` and `Trainer`.\n",
    "  6. Evaluate using `seqeval` (report precision, recall, F1, and accuracy) and run inference with a pipeline.\n",
    "\n",
    "- **Evaluation:** Use the `seqeval` metric (entity-level precision, recall, F1). When aligning predictions and labels, filter out `-100` entries so only real token labels are compared.\n",
    "\n",
    "- **Deliverables:** Completed notebook with working cells for data loading, tokenization/label alignment, training, evaluation, and an inference example. Add short comments explaining choices (e.g., sentence-splitting strategy, tokenizer settings).\n",
    "\n",
    "Good luck — implement each TODO in order and run the cells to verify output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b101905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install the required packages for Arabic NER with transformers\n",
    "# Required packages: transformers, datasets, seqeval, evaluate, accelerate\n",
    "# Use pip install with -q flag to suppress output\n",
    "\n",
    "!pip install transformers datasets seqeval evaluate accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abb573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.config', 'arabert-ner', 'sample_data']\n"
     ]
    }
   ],
   "source": [
    "# TODO: List the files in the current directory to explore the workspace\n",
    "# Hint: Use a simple command to display directory contents\n",
    "\n",
    "import os\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63aaa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da23007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 125102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 25008\n",
      "    })\n",
      "})\n",
      "Sample Entry: {'word': 'فرانكفورت', 'tag': 'B-LOC'}\n",
      "\n",
      "Label List: ['B-LOC', 'B-MISC', 'B-ORG', 'B-PERS', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PERS', 'O']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the ANERCorp dataset and extract label mappings\n",
    "# Steps:\n",
    "# 1. Import required libraries (datasets, numpy)\n",
    "# 2. Load the \"asas-ai/ANERCorp\" dataset using load_dataset()\n",
    "# 3. Inspect the dataset structure - print the splits and a sample entry\n",
    "# 4. Extract unique tags from the training split\n",
    "# 5. Create label_list (sorted), id2label, and label2id mappings\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# dataset = \n",
    "# print(f\"Dataset Split: {dataset}\")\n",
    "# print(f\"Sample Entry: {dataset['train'][0]}\")\n",
    "# unique_tags = \n",
    "# label_list = \n",
    "# id2label = \n",
    "# label2id = \n",
    "# print(f\"\\nLabel List: {label_list}\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"asas-ai/ANERCorp\")\n",
    "\n",
    "print(f\"Dataset Split: {dataset}\")\n",
    "print(f\"Sample Entry: {dataset['train'][0]}\")\n",
    "\n",
    "unique_tags = sorted(list(set(dataset[\"train\"][\"tag\"])))\n",
    "label_list = unique_tags\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "print(f\"\\nLabel List: {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6aa5b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 125102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 25008\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tag",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "803c986e-f0fd-4855-b92a-2e1dd4f59ce3",
       "rows": [
        [
         "0",
         "فرانكفورت",
         "B-LOC"
        ],
        [
         "1",
         "(د",
         "O"
        ],
        [
         "2",
         "ب",
         "O"
        ],
        [
         "3",
         "أ)",
         "O"
        ],
        [
         "4",
         "أعلن",
         "O"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c45307ec-3952-4a2c-a70c-2c2fca146e8a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>فرانكفورت</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(د</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ب</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>أ)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>أعلن</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c45307ec-3952-4a2c-a70c-2c2fca146e8a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c45307ec-3952-4a2c-a70c-2c2fca146e8a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c45307ec-3952-4a2c-a70c-2c2fca146e8a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        word    tag\n",
       "0  فرانكفورت  B-LOC\n",
       "1         (د      O\n",
       "2          ب      O\n",
       "3         أ)      O\n",
       "4       أعلن      O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Verify the dataset was loaded correctly\n",
    "# Print the dataframe or dataset summary to inspect the data structure\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(dataset)\n",
    "dataset[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be78b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and create tokenization function\n",
    "# Steps:\n",
    "# 1. Import AutoTokenizer from transformers\n",
    "# 2. Set model_checkpoint to \"aubmindlab/bert-base-arabertv02\"\n",
    "# 3. Load the tokenizer using AutoTokenizer.from_pretrained()\n",
    "# 4. Create tokenize_and_align_labels function that:\n",
    "#    - Tokenizes the input text (is_split_into_words=True)\n",
    "#    - Maps tokens to their original words\n",
    "#    - Handles special tokens by setting them to -100\n",
    "#    - Aligns labels with sub-word tokens\n",
    "#    - Returns tokenized inputs with labels\n",
    "# 5. Important: Convert words to sentences using punctuation marks \".?!\" as sentence delimiters\n",
    "#    - This helps the model understand sentence boundaries\n",
    "#    - Hint (suggested approach): group `examples['word']` into sentence lists using \".?!\" as end markers, e.g.:\n",
    "#        sentences = []\n",
    "#        current = []\n",
    "#        for w in examples['word']:\n",
    "#            current.append(w)\n",
    "#            if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "#                sentences.append(current)\n",
    "#                current = []\n",
    "#        if current:\n",
    "#            sentences.append(current)\n",
    "#      Then align `examples['tag']` accordingly to these sentence groups before tokenization.\n",
    "# 6. Apply the function to the entire dataset using dataset.map()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# model_checkpoint = \n",
    "# tokenizer = \n",
    "\n",
    "# def tokenize_and_align_labels(examples):\n",
    "#     # TODO: Implement tokenization and label alignment\n",
    "#     # Hint: Use tokenizer with is_split_into_words=True\n",
    "#     # Handle -100 for special tokens and sub-words\n",
    "#     # Note: Consider punctuation marks \".?!\" when processing sentence boundaries\n",
    "#     pass\n",
    "\n",
    "# tokenized_datasets = \n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def group_into_sentences(examples):\n",
    "    sentences, sentence_tags = [], []\n",
    "    current_sent, current_tags = [], []\n",
    "    \n",
    "    for w, t in zip(examples['word'], examples['tag']):\n",
    "        current_sent.append(w)\n",
    "        current_tags.append(label2id[t])\n",
    "        \n",
    "        if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "            sentences.append(current_sent)\n",
    "            sentence_tags.append(current_tags)\n",
    "            current_sent, current_tags = [], []\n",
    "    \n",
    "    if current_sent:\n",
    "        sentences.append(current_sent)\n",
    "        sentence_tags.append(current_tags)\n",
    "        \n",
    "    return {\"tokens\": sentences, \"ner_tags\": sentence_tags}\n",
    "\n",
    "grouped_dataset = dataset.map(group_into_sentences, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = grouped_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5b09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the compute_metrics function for model evaluation\n",
    "# Steps:\n",
    "# 1. Import evaluate and load \"seqeval\" metric\n",
    "# 2. Create compute_metrics function that:\n",
    "#    - Extracts predictions from model outputs using argmax\n",
    "#    - Filters out -100 labels (special tokens and sub-words)\n",
    "#    - Converts prediction and label IDs back to label names\n",
    "#    - Computes seqeval metrics (precision, recall, f1, accuracy)\n",
    "#    - Returns results as a dictionary\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# seqeval = \n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     # TODO: Implement metric computation\n",
    "#     # Hint: Use np.argmax, filter -100 labels, use seqeval.compute()\n",
    "#     pass\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e6e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-605633858.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260118_064126-v3ukyi0q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='822' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [822/822 04:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.138214</td>\n",
       "      <td>0.808274</td>\n",
       "      <td>0.795186</td>\n",
       "      <td>0.801677</td>\n",
       "      <td>0.966411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.145595</td>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.782932</td>\n",
       "      <td>0.812259</td>\n",
       "      <td>0.967810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.832276</td>\n",
       "      <td>0.803501</td>\n",
       "      <td>0.817635</td>\n",
       "      <td>0.968930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=822, training_loss=0.08221405033937626, metrics={'train_runtime': 274.2819, 'train_samples_per_second': 47.951, 'train_steps_per_second': 2.997, 'total_flos': 598472770007808.0, 'train_loss': 0.08221405033937626, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the model and configure training\n",
    "# Steps:\n",
    "# 1. Import AutoModelForTokenClassification, TrainingArguments, Trainer, and DataCollatorForTokenClassification\n",
    "# 2. Load the model using AutoModelForTokenClassification.from_pretrained() with:\n",
    "#    - model_checkpoint\n",
    "#    - num_labels based on label_list length\n",
    "#    - id2label and label2id mappings\n",
    "# 3. Create TrainingArguments with:\n",
    "#    - output directory \"arabert-ner\"\n",
    "#    - evaluation_strategy=\"epoch\"\n",
    "#    - learning_rate=2e-5\n",
    "#    - batch_size=16 (both train and eval)\n",
    "#    - num_train_epochs=3\n",
    "#    - weight_decay=0.01\n",
    "# 4. Create a DataCollatorForTokenClassification for dynamic padding\n",
    "# 5. Initialize the Trainer with model, args, datasets, data_collator, tokenizer, and compute_metrics\n",
    "# 6. Call trainer.train() to start training\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# model = \n",
    "# args = \n",
    "# data_collator = \n",
    "# trainer = \n",
    "# trainer.train()\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"arabert-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "496b7ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: أبل, Label: ORG, Score: 0.95\n",
      "Entity: تيم كوك, Label: PERS, Score: 0.99\n",
      "Entity: الرياض, Label: LOC, Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test the trained model with inference\n",
    "# Steps:\n",
    "# 1. Import pipeline from transformers\n",
    "# 2. Create an NER pipeline using the trained model and tokenizer\n",
    "# 3. Use aggregation_strategy=\"simple\" to merge sub-tokens back into words\n",
    "# 4. Test the pipeline with an Arabic text sample\n",
    "# 5. Pretty print the results showing entity, label, and confidence score\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# ner_pipeline = \n",
    "# \n",
    "# text = \"أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\"\n",
    "# results = \n",
    "#\n",
    "# # Pretty print results\n",
    "# for entity in results:\n",
    "#     print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")\n",
    "\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\"\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "for entity in results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
